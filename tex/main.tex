\documentclass[a4paper, 12pt]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[T1]{fontenc}
\usepackage{Alegreya}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{enumitem}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{calc}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\title{\scshape Implementing the Truncated Density Convolution}
\author{\normalsize Kangbo Li \& Jalen Harris}
\date{\normalsize \today}
\begin{document}
\maketitle

\section{Theory Overview}%
\label{sec:Theory Overview}

This document describes the details of what we want to implement. 
The optimization problem can be written in abstract as 
\begin{align}
        \min_{U}\; &  \Omega(U),\\
        \Omega(U) = 2 \sum_{n, b} w_b (1 - |\hat{\rho}_n(b)|),& \quad \hat{\rho}_n(b) = \frac{1}{N_k}  \sum_k (U^{k \dagger} S^{k, b} U^{\mathrm{add}(k, b)})_{n,n},\\
        \mathrm{s.t}\; & U^{k \dagger} U^{k} = I,\\
        U \in \mathbb{C}^{N \times N \times N_k },& S \in \mathbb{C}^{N \times N \times N_b \times N_k}, 
        w \in \mathbb{R}^{N_b}\label{eq:dimensions}.
\end{align}
This is a constrained minimization problem, where the independent variable  is
a complex tensor. The gradient of the objective function is 
\begin{equation}
    (\nabla \Omega(U))^k_{ij} = - \frac{4}{N_k}  \sum_{b, s} w_b \frac{\hat{\rho}_j^*(b)}{|\hat{\rho}_j(b)|} S_{i, s}^{k, b} U_{s, j}^{\mathrm{add}(k, b)}.
\end{equation}
The gist of the project is to make $\Omega$ and $\nabla \Omega$ fast. The
$\mathrm{add}$ function is  a world of hurt, so we will just tabulate the
result of the reference code and not worry about it.

This constrained optimization problem is solved with a manifold conjugate
gradient descent. The algorithm for a nonlinear conjugate gradient is roughly
\vspace{-0.5cm}
\begin{verbatim}
    function cg(f, grad_f)
        x = zeros
        p = grad_f(x)
        while not converged
            x = x - alpha * p
            p = grad_f(x) + beta * p
        end
    end
\end{verbatim}
\vspace{-0.5cm}
The step length \texttt{alpha} is determined by a (quadratic) line search
whereas the formula for \texttt{beta} has many variants.  The one implemented
in the reference code is the Fletcher Reeves.

It would be ideal if we could use an off-the-shelf optimizer, but the complication is 
that we are working on a manifold, which modifies the algorithm
\vspace{-0.5cm}
\begin{verbatim}
    function cg(f, grad_f)
        x = zeros
        p = project(grad_f(x))
        while not converged
            x = retract(x, -alpha * p)
            p = project(grad_f(x)) + beta * p
        end
    end
\end{verbatim}
\vspace{-0.5cm}
The projection and the retraction implicitly used in Wannier90 are
\begin{align}
    \mathrm{project}(X) &= (X - X^{\dagger}) / 2,\\
    \mathrm{retract}(U, \Delta U) &= k \to U^k \exp(U^{k \dagger} \Delta U^k).
\end{align}
The purpose of the retraction is to enforce the constraint, so the exponential
is unnecessarily expensive  and a QR of $U + \Delta U$ is likely fine.

\section{Implementation Plan}%
\label{sec:Implementation Plan}

\begin{enumerate}
    \item (x) Prepare the parameters of the problem.
    \item Read the parameters from Fortran.
    \item Serial implementation of the objective function, the gradient, and
        the retraction.
    \item Implement a manifold optimization.
    \item Speedup.
\end{enumerate}

\subsection{Prepare the data}%
\label{sub:Prepare the data}

The $S$ and $w$ are written in a Fortran binary file just as raw Fortran arrays
whose dimensions match Eq.~\ref{eq:dimensions}. The $\mathbf{add}$ function is 
tabulated and saved to a Fortran binary file as a $N_k \times N_b$ array, 
where the $k^{th}$ row and $b^{th}$ column is the new index $\mathrm{add}(k, b)$.

\end{document}
